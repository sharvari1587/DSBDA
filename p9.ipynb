{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.\n",
    "For given text apply following preprocessing methods:\n",
    "\n",
    "1. Tokenization\n",
    "2. POS Tagging\n",
    "3. Stop word Removal\n",
    "4. Lemmatization\n",
    "5. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'The', 'dogs', 'are', 'barking', 'loudly', '.']\n",
      "POS Tagging: [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.'), ('The', 'DT'), ('dogs', 'NNS'), ('are', 'VBP'), ('barking', 'VBG'), ('loudly', 'RB'), ('.', '.')]\n",
      "Stop word Removal: ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog', '.', 'dogs', 'barking', 'loudly', '.']\n",
      "Lemmatization: ['quick', 'brown', 'fox', 'jump', 'lazy', 'dog', '.', 'dog', 'barking', 'loudly', '.']\n",
      "Stemming: ['quick', 'brown', 'fox', 'jump', 'lazi', 'dog', '.', 'dog', 'bark', 'loudli', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# Sample text\n",
    "text = \"The quick brown fox jumps over the lazy dog. The dogs are barking loudly.\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokenization:\", tokens)\n",
    "\n",
    "# POS Tagging\n",
    "pos_tags = pos_tag(tokens)       #determin adjective verb noun\n",
    "print(\"POS Tagging:\", pos_tags)\n",
    "\n",
    "# Stop word removal\n",
    "stop_words = set(stopwords.words('english'))         #the uniformative word which doest not add substance to the sentence \n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(\"Stop word Removal:\", filtered_tokens)\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(\"Lemmatization:\", lemmatized_tokens)\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(\"Stemming:\", stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import necessary modules from NLTK including tokenization, POS tagging, stopwords, lemmatization, and stemming.\n",
    "We define a sample text.\n",
    "We tokenize the text into individual words.\n",
    "We perform POS tagging to assign a part-of-speech tag to each word.\n",
    "We remove stop words (commonly occurring words like \"the\", \"is\", \"are\", etc.).\n",
    "We lemmatize the words to reduce them to their base or dictionary form.\n",
    "We stem the words to remove affixes and reduce them to their root form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, let me explain each of these text preprocessing techniques:\n",
    "\n",
    "1. **Tokenization**:\n",
    "   - Tokenization is the process of breaking down a text into smaller units called tokens. These tokens can be words, phrases, or other meaningful elements.\n",
    "   - For example, consider the sentence: \"The quick brown fox jumps over the lazy dog.\" Tokenization would split this sentence into individual words: [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\", \".\"].\n",
    "   - Tokenization is a crucial step in natural language processing (NLP) tasks as it helps in extracting meaningful information from text data.\n",
    "\n",
    "2. **POS Tagging** (Part-of-Speech Tagging):\n",
    "   - POS tagging is the process of assigning a part-of-speech tag (such as noun, verb, adjective, etc.) to each word in a sentence.\n",
    "   - This helps in understanding the grammatical structure of a sentence and aids in various NLP tasks such as named entity recognition, text summarization, etc.\n",
    "   - For example, consider the sentence: \"The quick brown fox jumps over the lazy dog.\" POS tagging would assign tags to each word: [(\"The\", \"DT\"), (\"quick\", \"JJ\"), (\"brown\", \"JJ\"), (\"fox\", \"NN\"), (\"jumps\", \"VBZ\"), (\"over\", \"IN\"), (\"the\", \"DT\"), (\"lazy\", \"JJ\"), (\"dog\", \"NN\"), (\".\", \".\")].\n",
    "   - Here, \"DT\" represents determiner, \"JJ\" represents adjective, \"NN\" represents noun, \"VBZ\" represents verb (3rd person singular present), and \"IN\" represents preposition or conjunction.\n",
    "\n",
    "3. **Stop Word Removal**:\n",
    "   - Stop words are common words that are often considered irrelevant for text analysis as they do not carry much meaning (e.g., \"the\", \"is\", \"are\", \"and\", \"but\", etc.).\n",
    "   - Stop word removal is the process of filtering out these stop words from text data.\n",
    "   - This helps in reducing noise in the data and improving the efficiency of text processing algorithms.\n",
    "   - For example, consider the sentence: \"The quick brown fox jumps over the lazy dog.\" After stop word removal, it would become: [\"quick\", \"brown\", \"fox\", \"jumps\", \"lazy\", \"dog\"].\n",
    "\n",
    "4. **Lemmatization**:\n",
    "   - Lemmatization is the process of reducing words to their base or dictionary form (known as lemma) while still ensuring that the reduced form belongs to the language.\n",
    "   - It involves removing inflections and variations to bring words to their root form.\n",
    "   - For example, the lemma of the words \"running\", \"ran\", and \"runs\" is \"run\".\n",
    "   - Lemmatization helps in standardizing words so that variations of the same word are treated as the same entity in text analysis.\n",
    "\n",
    "5. **Stemming**:\n",
    "   - Stemming is similar to lemmatization, but it is a more crude and rule-based approach.\n",
    "   - It involves removing prefixes and suffixes from words to reduce them to their root or stem form.\n",
    "   - Stemming is more aggressive than lemmatization and may not always result in valid words.\n",
    "   - For example, stemming the words \"running\", \"ran\", and \"runs\" would result in \"run\".\n",
    "   - Stemming is computationally less expensive compared to lemmatization and is often used in information retrieval systems and text mining tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
